# GitHub Actions workflow for AWS Glue ETL CI/CD
name: Glue ETL CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'main'
        

jobs:
  test:
    name: Test ETL Job
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pandas pyspark delta mock
          
      
      - name: Run unit tests
        run: |
          pytest tests/ --cov=src/ --cov-report=xml -v
      
      - name: Upload test coverage report
        uses: actions/upload-artifact@v3
        with:
          name: test-coverage-report
          path: coverage.xml
          
      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository

  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort
          
      - name: Run linters
        run: |
          # Check code formatting with black
          black --check src/ tests/
          # Check import sorting
          isort --check-only --profile black src/ tests/
          # Run flake8 for style guide enforcement
          flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics

  build:
    name: Build ETL Package
    needs: [test, lint]
    runs-on: ubuntu-latest
    if: success() && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Package Glue job
        run: |
          mkdir -p dist
          # Package the main ETL script
          cp src/etl_script.py dist/
          # Package any additional Python dependencies
          if [ -d "src/dependencies" ]; then
            zip -r dist/dependencies.zip src/dependencies/
          fi
          
      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: etl-package
          path: |
            dist/
            
  deploy-dev:
    name: Deploy to Dev
    needs: [build]
    runs-on: ubuntu-latest
    if: success() && (github.ref == 'refs/heads/develop' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'dev'))
    environment: dev
    
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: etl-package
          path: dist
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Deploy ETL job to S3
        run: |
          # Upload ETL script to S3
          aws s3 cp dist/etl_script.py s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/dev/
          # Upload dependencies if they exist
          if [ -f "dist/dependencies.zip" ]; then
            aws s3 cp dist/dependencies.zip s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/dev/
          fi
          
      - name: Update Glue job
        run: |
          # Check if Glue job exists
          if aws glue get-job --job-name etl-job-dev; then
            # Update existing job
            aws glue update-job --job-name etl-job-dev \
              --job-update "{ \
                \"Role\": \"${{ secrets.GLUE_JOB_ROLE }}\", \
                \"Command\": { \
                  \"Name\": \"glueetl\", \
                  \"ScriptLocation\": \"s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/dev/etl_script.py\", \
                  \"PythonVersion\": \"3\" \
                }, \
                \"DefaultArguments\": { \
                  \"--job-language\": \"python\", \
                  \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                  \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/dev\", \
                  \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/dev\", \
                  \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/dev\", \
                  \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/dev\", \
                  \"--LOG_LEVEL\": \"INFO\", \
                  \"--enable-metrics\": \"\", \
                  \"--enable-continuous-cloudwatch-log\": \"true\", \
                  \"--job-bookmark-option\": \"job-bookmark-enable\" \
                }, \
                \"ExecutionProperty\": { \
                  \"MaxConcurrentRuns\": 1 \
                }, \
                \"GlueVersion\": \"5.0\", \
                \"NumberOfWorkers\": 2, \
                \"WorkerType\": \"G.1X\" \
              }"
          else
            # Create new job
            aws glue create-job --name etl-job-dev \
              --role ${{ secrets.GLUE_JOB_ROLE }} \
              --command "Name=glueetl,ScriptLocation=s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/dev/etl_script.py,PythonVersion=3" \
              --default-arguments "{ \
                \"--job-language\": \"python\", \
                \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/dev\", \
                \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/dev\", \
                \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/dev\", \
                \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/dev\", \
                \"--LOG_LEVEL\": \"INFO\", \
                \"--enable-metrics\": \"\", \
                \"--enable-continuous-cloudwatch-log\": \"true\", \
                \"--job-bookmark-option\": \"job-bookmark-enable\" \
              }" \
              --execution-property MaxConcurrentRuns=1 \
              --glue-version "4.0" \
              --number-of-workers 2 \
              --worker-type "G.1X"
          fi
          
  deploy-test:
    name: Deploy to Test
    needs: [deploy-dev]
    runs-on: ubuntu-latest
    if: success() && (github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'test'))
    environment: test
    
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: etl-package
          path: dist
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Deploy ETL job to S3
        run: |
          # Upload ETL script to S3
          aws s3 cp dist/etl_script.py s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/test/
          # Upload dependencies if they exist
          if [ -f "dist/dependencies.zip" ]; then
            aws s3 cp dist/dependencies.zip s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/test/
          fi
          
      - name: Update Glue job
        run: |
          # Check if Glue job exists
          if aws glue get-job --job-name etl-job-test; then
            # Update existing job
            aws glue update-job --job-name etl-job-test \
              --job-update "{ \
                \"Role\": \"${{ secrets.GLUE_JOB_ROLE }}\", \
                \"Command\": { \
                  \"Name\": \"glueetl\", \
                  \"ScriptLocation\": \"s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/test/etl_script.py\", \
                  \"PythonVersion\": \"3\" \
                }, \
                \"DefaultArguments\": { \
                  \"--job-language\": \"python\", \
                  \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                  \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/test\", \
                  \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/test\", \
                  \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/test\", \
                  \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/test\", \
                  \"--LOG_LEVEL\": \"INFO\", \
                  \"--enable-metrics\": \"\", \
                  \"--enable-continuous-cloudwatch-log\": \"true\", \
                  \"--job-bookmark-option\": \"job-bookmark-enable\" \
                }, \
                \"ExecutionProperty\": { \
                  \"MaxConcurrentRuns\": 1 \
                }, \
                \"GlueVersion\": \"5.0\", \
                \"NumberOfWorkers\": 2, \
                \"WorkerType\": \"G.1X\" \
              }"
          else
            # Create new job
            aws glue create-job --name etl-job-test \
              --role ${{ secrets.GLUE_JOB_ROLE }} \
              --command "Name=glueetl,ScriptLocation=s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/test/etl_script.py,PythonVersion=3" \
              --default-arguments "{ \
                \"--job-language\": \"python\", \
                \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/test\", \
                \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/test\", \
                \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/test\", \
                \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/test\", \
                \"--LOG_LEVEL\": \"INFO\", \
                \"--enable-metrics\": \"\", \
                \"--enable-continuous-cloudwatch-log\": \"true\", \
                \"--job-bookmark-option\": \"job-bookmark-enable\" \
              }" \
              --execution-property MaxConcurrentRuns=1 \
              --glue-version "5.0" \
              --number-of-workers 2 \
              --worker-type "G.1X"
          fi
          
  deploy-prod:
    name: Deploy to Production
    needs: [deploy-test]
    runs-on: ubuntu-latest
    if: success() && (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'prod')
    environment: prod
    
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: etl-package
          path: dist
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Deploy ETL job to S3
        run: |
          # Upload ETL script to S3
          aws s3 cp dist/etl_script.py s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/prod/
          # Upload dependencies if they exist
          if [ -f "dist/dependencies.zip" ]; then
            aws s3 cp dist/dependencies.zip s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/prod/
          fi
          
      - name: Update Glue job
        run: |
          # Check if Glue job exists
          if aws glue get-job --job-name etl-job-prod; then
            # Update existing job
            aws glue update-job --job-name etl-job-prod \
              --job-update "{ \
                \"Role\": \"${{ secrets.GLUE_JOB_ROLE }}\", \
                \"Command\": { \
                  \"Name\": \"glueetl\", \
                  \"ScriptLocation\": \"s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/prod/etl_script.py\", \
                  \"PythonVersion\": \"3\" \
                }, \
                \"DefaultArguments\": { \
                  \"--job-language\": \"python\", \
                  \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                  \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/prod\", \
                  \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/prod\", \
                  \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/prod\", \
                  \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/prod\", \
                  \"--LOG_LEVEL\": \"INFO\", \
                  \"--enable-metrics\": \"\", \
                  \"--enable-continuous-cloudwatch-log\": \"true\", \
                  \"--job-bookmark-option\": \"job-bookmark-enable\" \
                }, \
                \"ExecutionProperty\": { \
                  \"MaxConcurrentRuns\": 2 \
                }, \
                \"GlueVersion\": \"5.0\", \
                \"NumberOfWorkers\": 2, \
                \"WorkerType\": \"G.1X\", \
                \"Timeout\": 180 \
              }"
          else
            # Create new job
            aws glue create-job --name etl-job-prod \
              --role ${{ secrets.GLUE_JOB_ROLE }} \
              --command "Name=glueetl,ScriptLocation=s3://${{ secrets.GLUE_SCRIPTS_BUCKET }}/etl/prod/etl_script.py,PythonVersion=3" \
              --default-arguments "{ \
                \"--job-language\": \"python\", \
                \"--S3_BUCKET_PATH\": \"${{ secrets.DATA_SOURCE_PATH }}\", \
                \"--ORDER_ITEMS_OUTPUT_PATH\": \"${{ secrets.ORDER_ITEMS_OUTPUT_PATH }}/prod\", \
                \"--ORDERS_OUTPUT_PATH\": \"${{ secrets.ORDERS_OUTPUT_PATH }}/prod\", \
                \"--PRODUCTS_OUTPUT_PATH\": \"${{ secrets.PRODUCTS_OUTPUT_PATH }}/prod\", \
                \"--REJECTED_PATH\": \"${{ secrets.REJECTED_PATH }}/prod\", \
                \"--LOG_LEVEL\": \"INFO\", \
                \"--enable-metrics\": \"\", \
                \"--enable-continuous-cloudwatch-log\": \"true\", \
                \"--job-bookmark-option\": \"job-bookmark-enable\" \
              }" \
              --execution-property MaxConcurrentRuns=2 \
              --glue-version "5.0" \
              --number-of-workers 2 \
              --worker-type "G.1X" \
              --timeout 180
          fi

  integration-test:
    name: Run Integration Tests
    needs: [deploy-test]
    runs-on: ubuntu-latest
    if: success() && github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'prod'
    environment: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 pytest-bdd pytest
          if [ -f requirements-integration.txt ]; then pip install -r requirements-integration.txt; fi
          
      - name: Run integration tests against test environment
        run: |
          # Generate test data in S3
          python integration_tests/generate_test_data.py --bucket ${{ secrets.TEST_DATA_BUCKET }}
          
          # Start Glue job and wait for completion
          python integration_tests/run_glue_job.py --job-name etl-job-test
          
          # Run integration tests against the output
          pytest integration_tests/test_integration.py --junitxml=integration-test-results.xml
          
      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results
          path: integration-test-results.xml
